{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "164f8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706be79",
   "metadata": {},
   "source": [
    "Per capire come funziona Autograd, definisco un tensore a richiedendo che venga accumulato il gradiente, cioè scrivendo requires_grad=True. Quindi ogni update su a verrà calcolato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03debd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37087025",
   "metadata": {},
   "source": [
    "Ora definisco una prima funzione, $Q(a)=a^2$, per la quale la derivata è\n",
    "$$\\frac{\\textrm{d}Q}{\\textrm{d}a}=2a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72bcbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = a**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87857f36",
   "metadata": {},
   "source": [
    "Quando chiamo backward(), autograd calcola questo gradiente e lo immagazzina in a.grad. Quindi mi aspetto che a.grad mi restituisca $2\\cdot a=2\\cdot (2,3) = (4,6)$.\n",
    "\n",
    "**Nota bene:** a patto che a non sia uno scalare, devo specificare un vettore v da mettere nel parametro gradient. Metto (1.,1.) perché verrà moltiplicato per il gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda3c173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3.], requires_grad=True), tensor([4., 6.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([1.,1.])   \n",
    "Q.backward(gradient = v)\n",
    "\n",
    "a,a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb23407",
   "metadata": {},
   "source": [
    "Ora eseguo una seconda funzione $P(a)=3a$ e, quando chiamo backward, viene immagazzinato il gradiente in a.grad, sommandolo a quello esistente. Quindi mi aspetto che a.grad ora mi restituisca il vecchio gradiente più 3: $(4,6)+(3,3)=(7,9)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d55dc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3.], requires_grad=True), tensor([7., 9.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = 3*a\n",
    "P.backward(gradient=v)\n",
    "a,a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf34a6",
   "metadata": {},
   "source": [
    "Se ora calcolo una terza funzione $R(a)=a^3$ e chiamo backward, mi aspetto che a.grad restituisca $(7,9) -3a^2=(7,9)-3*(4,9)=(7,9)-(12,27)=(-5,-18)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31dacb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3.], requires_grad=True), tensor([ -5., -18.]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R=-a**3\n",
    "R.backward(gradient=v)\n",
    "a,a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f569a",
   "metadata": {},
   "source": [
    "Se definisco un'ulteriore funzione, senza chiamare backward, a.grad non viene aggiornato. Meglio, chiamo il decorator torch.no_grad() così direttamente non lo calcola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d73b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3.], requires_grad=True), tensor([ -5., -18.]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    S = 5*a\n",
    "a,a.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
